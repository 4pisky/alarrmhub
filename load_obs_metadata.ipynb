{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import driveami\n",
    "import datetime\n",
    "from driveami import keys as amikeys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ami_utc_to_datetimes(utc_str):\n",
    "    \"\"\"Build 'datetime' objects from AMI-format date-strings\"\"\"\n",
    "    date_format = '%Y-%m-%d %H:%M:%S'\n",
    "    return datetime.datetime.strptime(utc_str, date_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = './'\n",
    "with open(os.path.join(data_dir,'all_ami_rawfiles_metadata.json')) as f:\n",
    "    observations, _ = driveami.load_listing(f)\n",
    "with open(os.path.join(data_dir,'all_ami_rawfiles_by_pointing.json')) as f:\n",
    "    groupings, _ = driveami.load_listing(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Loaded data for\", len(observations), \"observations, grouped into\", len(groupings), \"pointings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Drop rawtext - this is a big bunch of raw text data output from the AMI-REDUCE pipeline \n",
    "#which is mainly just used for debugging - we're not interested in it here.\n",
    "for k in observations.keys():\n",
    "    observations[k].pop(amikeys.raw_obs_text, None)\n",
    "\n",
    "#Add the group ID to each observation entry \n",
    "for group_id, group_dict in groupings.iteritems():\n",
    "    for filename in group_dict[amikeys.files]:\n",
    "        observations[filename][amikeys.group_name]= group_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we have a big dictionary, ``observations``, which contains basically all the information about the AMI dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "observations.keys()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#e.g.\n",
    "print(observations.keys()[0])\n",
    "observations.values()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can process this raw data however you like. Below I've shown a couple of quick examples of how you can use the [pandas](http://pandas.pydata.org/) library to quickly manipulate the dataset, and display it in tabular form. First, let's simply load the data into a ``DataFrame``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame.from_records(observations).T\n",
    "df = df.drop([amikeys.comment,amikeys.pointing_hms_dms,amikeys.field],1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a bad start, but it would be nicer if we could separate out the start/end dates into separate columns. Let's do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['utc_start'] = df.time_utc.apply(lambda x: ami_utc_to_datetimes(x[0]))\n",
    "df['utc_end'] = df.time_utc.apply(lambda x: ami_utc_to_datetimes(x[1]))\n",
    "df['mjd_start'] = df.time_mjd.apply(lambda x: x[0])\n",
    "df['mjd_end'] = df.time_mjd.apply(lambda x: x[1])\n",
    "df['ra'] = df.pointing_degrees.apply(lambda x: x[0])\n",
    "df['dec'] = df.pointing_degrees.apply(lambda x: x[1])\n",
    "#Make a new DataFrame, 'obs', without the old columns, or the warning_flags:\n",
    "obs = df.drop([amikeys.time_ut, amikeys.time_mjd, amikeys.pointing_degrees, amikeys.warnings], 1)\n",
    "obs = obs.sort('utc_start')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "obs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Now we can use our new DataFrame to filter down the dataset and just show us what we're interested in.\n",
    "For example, what if we just want to list observations made in the last week?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "recent_obs = obs[(now - obs.utc_start) < datetime.timedelta(days=7)]\n",
    "recent_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or how about just listing V404 observations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v404 = obs[obs.group_name=='SWIFT_643949'].sort('mjd_start')\n",
    "v404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can convert this back to a basic Python dictionary if required:\n",
    "v404.T.to_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
